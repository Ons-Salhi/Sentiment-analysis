# -*- coding: utf-8 -*-
"""SentimentAnalysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Vq24enA2Ft7-gnJWyfwRZrnkmfcHS0Ru
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
plt.style.use('ggplot') #a style sheet
import nltk

#Reading our data
df = pd.read_csv('/content/Reviews.csv')
print(df.shape)
df = df.head(500) #trying to scale down the project a bit
print(df.shape)

df.head()

# We want to see the 1st text a bit clearer 
df['Text'].values[0]

"""#Quick Exploratory Data Analysis """

#Value counts gives us the num of times each score occurs
ax = df['Score'].value_counts().sort_index().plot(kind='bar',
                                             title='Count of reviews by stars',
                                             figsize=(10, 5))
ax.set_xlabel('Review Stars')
plt.show()

"""#Trying out some basic NLTK"""

import nltk
nltk.download('averaged_perceptron_tagger')
nltk.download('maxent_ne_chunker')
nltk.download('words')

ex = df['Text'][50]
print(ex)

tokens = nltk.word_tokenize(ex) #tokenizing our result
tokens[:10]

tagged = nltk.pos_tag(tokens) #looking for the part of the speech for each of these words
tagged[:10] #just taking the first 10

entities = nltk.chunk.ne_chunk(tagged) #This takes the tokens and groups them into chunks of text
entities.pprint()

"""#VADER sentiment analysis"""

from nltk.sentiment import SentimentIntensityAnalyzer
from tqdm.notebook import tqdm #A progress bar tracker

sia = SentimentIntensityAnalyzer()
sia.polarity_scores('I am so happy!')

sia.polarity_scores('This is the worst thing i ever bought')

sia.polarity_scores(ex) #Trying to run the polarity score on our previous example

"""#Running the polarity score on our entire dataset"""

res = {}
for i, row in tqdm(df.iterrows(), total=len(df)):
    text = row['Text']
    myid = row['Id']
    res[myid]= sia.polarity_scores(text)

vaders = pd.DataFrame(results).T #The .T makes the df horizontal
vaders

vaders = vaders.reset_index().rename(columns={'index': 'Id'}) #Renaming our index as our id so we can merge this into our original df
vaders = vaders.merge(df, how='left')

#Now we have sentiment score and metadata
vaders.head()

"""#Plot VADER results"""

ax = sns.barplot(data=vaders, x='Score', y='compound')
ax.set_title('Compound Score by Amazon Star Review')
plt.show()

fig, axs = plt.subplots(1, 3, figsize=(12, 3))
sns.barplot(data=vaders, x='Score', y='pos', ax=axs[0])
sns.barplot(data=vaders, x='Score', y='neu', ax=axs[1])
sns.barplot(data=vaders, x='Score', y='neg', ax=axs[2])
axs[0].set_title('Positive')
axs[1].set_title('Neutral')
axs[2].set_title('Negative')
plt.tight_layout()
plt.show()

"""#Roberta Pretrained Model"""

!pip install transformers

from transformers import AutoTokenizer
from transformers import AutoModelForSequenceClassification
from scipy.special import softmax

MODEL = f"cardiffnlp/twitter-roberta-base-sentiment" #this model was trained on a bunch of twitter comments that were labeled
tokenizer = AutoTokenizer.from_pretrained(MODEL)
model = AutoModelForSequenceClassification.from_pretrained(MODEL)

# VADER results on our example
print(ex)
sia.polarity_scores(ex)

# Run for Roberta Model
encoded_text = tokenizer(ex, return_tensors='pt') #encoding the text
output = model(**encoded_text)                    #running our model on our encoded text
scores = output[0][0].detach().numpy()            #making our results from tensor to numpy so we can store it locally
scores = softmax(scores)
scores_dict = {
    'roberta_neg' : scores[0],
    'roberta_neu' : scores[1],
    'roberta_pos' : scores[2]
}
print(scores_dict)

#Running it on our entire dataset
def polarity_scores_roberta(example):
    encoded_text = tokenizer(example, return_tensors='pt')
    output = model(**encoded_text)
    scores = output[0][0].detach().numpy()
    scores = softmax(scores)
    scores_dict = {
        'roberta_neg' : scores[0],
        'roberta_neu' : scores[1],
        'roberta_pos' : scores[2]
    }
    return scores_dict

res = {}
for i, row in tqdm(df.iterrows(), total=len(df)):
    try:                                                  #added an exception bc it broke for two examples
        text = row['Text']
        myid = row['Id']
        vader_result = sia.polarity_scores(text)
        vader_result_rename = {}
        for key, value in vader_result.items():
            vader_result_rename[f"vader_{key}"] = value
        roberta_result = polarity_scores_roberta(text)
        both = {**vader_result_rename, **roberta_result}  #combining both dict
        res[myid] = both
    except RuntimeError:            
        print(f'Broke for id {myid}')

results_df = pd.DataFrame(res).T
results_df = results_df.reset_index().rename(columns={'index': 'Id'})
results_df = results_df.merge(df, how='left')

results_df.head()

"""#Comparing scores between models"""

results_df.columns

"""#Combining and comparing"""

sns.pairplot(data=results_df,
             vars=['vader_neg', 'vader_neu', 'vader_pos',
                  'roberta_neg', 'roberta_neu', 'roberta_pos'],
            hue='Score',
            palette='tab10')
plt.show()

"""#Review Examples
where Positive 1-Star and Negative 5-Star Reviews
"""

results_df.query('Score == 1').sort_values('roberta_pos', ascending=False)['Text'].values[0]

results_df.query('Score == 1').sort_values('vader_pos', ascending=False)['Text'].values[0]

"""#Negative sentiment 5-Star view"""

results_df.query('Score == 5').sort_values('roberta_neg', ascending=False)['Text'].values[0]
#This is a negative sentiment but a positive review

results_df.query('Score == 5').sort_values('vader_neg', ascending=False)['Text'].values[0]

"""#The Transformers Pipeline



*   Quick & easy way to run sentiment predictions




"""

from transformers import pipeline
sent_pipeline = pipeline("sentiment-analysis")

sent_pipeline('I love learning this!')

sent_pipeline('I will have an amazing internship')

sent_pipeline('shut up')